{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Project Titanic - All Classification Algos Comparison.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n2XIR1xfRNMJ",
        "r6lr-jkCRNMM",
        "TyuT9CgRRNMr",
        "jqBOoDd-RNNZ",
        "qPeGPPsXRNNZ",
        "k92h_aBVRNNr",
        "l_Xb0yBrRNOe"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iOl7Qq6RNLH"
      },
      "source": [
        "# Classification Algorithms with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kygMZEyLRNLJ"
      },
      "source": [
        "For this lecture we will be working with the Titanic Data Set from Kaggle. This is a very famous data set and very often is a student's first step in machine learning!\n",
        "\n",
        "We'll be trying to predict a classification- survival or deceased. Let's begin our understanding of implementing Logistic Regression in Python for classification.\n",
        "\n",
        "We'll use a \"semi-cleaned\" version of the titanic data set, if you use the data set hosted directly on Kaggle, you may need to do some additional cleaning not shown here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0orQB04IRNLK"
      },
      "source": [
        "Import Libraries\n",
        "Let's import some libraries to get started!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF9nlGRnRNLP"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "# from IPython.core.interactiveshell import InteractiveShell  \n",
        "# InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "# # InteractiveShell.ast_node_interactivity = \"last_expr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9c5bMyJRNLS"
      },
      "source": [
        "# 2+3\n",
        "# 5-6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmpDsM2kRNLW"
      },
      "source": [
        "The Data\n",
        "Let's start by reading in the titanic_train.csv file into a pandas dataframe."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7dqqP7vRNLZ"
      },
      "source": [
        "train = pd.read_csv('titanic_train.csv')\n",
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNP6wibRNLd"
      },
      "source": [
        "train.describe()[['Pclass', 'Age']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwUNyfJ1RNLg"
      },
      "source": [
        "# !pip install pandas-profiling\n",
        "import pandas_profiling\n",
        "pandas_profiling.ProfileReport(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znryen5fRNLk"
      },
      "source": [
        "profile = pandas_profiling.ProfileReport(train)\n",
        "profile.to_file(output_file=\"Titanic data profiling.html\")\n",
        "train.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_IclkQBRNLo"
      },
      "source": [
        "# Exploratory Data Analysis¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hh_oVv6RNLo"
      },
      "source": [
        "Let's begin some exploratory data analysis! We'll start by checking out missing data!\n",
        "\n",
        "Missing Data\n",
        "We can use seaborn to create a simple heatmap to see where we are missing data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dV3R23CRNLp"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFfwhSCYRNLs"
      },
      "source": [
        "sns.heatmap(train.isnull(), yticklabels=False,\\\n",
        "            cbar=False,cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68-JCo59RNLv"
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.countplot(x='Survived',data=train,palette='RdBu_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgsJUg9TRNLz"
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.countplot(x='Survived',hue='Sex',data=train, palette='RdBu_r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZVUZZJGRNL2"
      },
      "source": [
        "sns.set_style('whitegrid')\n",
        "sns.countplot(x='Survived',hue='Pclass',data=train, palette='rainbow')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJnTwimcRNL5"
      },
      "source": [
        "sns.distplot(train['Age'].dropna(),kde=True, color='darkred',bins=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtdSuNkhRNL7"
      },
      "source": [
        "train['Age'].hist(bins=20,color='darkred',alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-9c98M7RNL-"
      },
      "source": [
        "sns.countplot(x='SibSp',data=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t2GzwwjRNME"
      },
      "source": [
        "train[train['Fare'] >200]['Survived'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32C6MpR3RNMG"
      },
      "source": [
        "\n",
        "train['Fare'].hist(color='green',bins=40,figsize=(8,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2XIR1xfRNMJ"
      },
      "source": [
        "# Cufflinks for plots¶\n",
        "Let's take a quick moment to show an example of cufflinks!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI3b1cZVRNMK"
      },
      "source": [
        "# !pip install cufflinks\n",
        "import cufflinks as cf\n",
        "cf.go_offline()\n",
        "train['Fare'].iplot(kind='hist',bins=30,color='green')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6lr-jkCRNMM"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdMiqAgzRNMN"
      },
      "source": [
        "We want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation). However we can be smarter about this and check the average age by passenger class. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WZRyOuNRNMN"
      },
      "source": [
        "plt.figure(figsize=(12, 7))\n",
        "sns.boxplot(x='Pclass',y='Age',data=train,palette='winter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byF9YcKqRNMQ"
      },
      "source": [
        "train[train['Pclass']==1]['Age'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT0UiC2GRNMS"
      },
      "source": [
        "train[train['Pclass']==1]['Age'].mean()\n",
        "train[train['Pclass']==2]['Age'].mean()\n",
        "train[train['Pclass']==3]['Age'].mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7P_XSP_RNMV"
      },
      "source": [
        "def impute_age(cols):\n",
        "    Age = cols[0]\n",
        "    Pclass = cols[1]\n",
        "    \n",
        "    if pd.isnull(Age):\n",
        "\n",
        "        if Pclass == 1:\n",
        "            return 38.23\n",
        "\n",
        "        elif Pclass == 2:\n",
        "            return 29.87\n",
        "\n",
        "        else:\n",
        "            return 25.140\n",
        "\n",
        "    else:\n",
        "        return Age"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lxo9iwFsRNMX"
      },
      "source": [
        "Now apply that function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qHHCNvLRNMY"
      },
      "source": [
        "train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44x6eK3kRNMb"
      },
      "source": [
        "train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-vxU5saRNMf"
      },
      "source": [
        "age_mean = train.groupby('Pclass')['Age'].mean()\n",
        "age_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgGJQwmPRNMi"
      },
      "source": [
        "sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbF8ULquRNMk"
      },
      "source": [
        "train.drop(['Name', 'Ticket', 'Cabin'], inplace=True, axis=1)\n",
        "train['Embarked'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wttbyStcRNMm"
      },
      "source": [
        "train.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyuT9CgRRNMr"
      },
      "source": [
        "# Converting Categorical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2negiN8RNMr"
      },
      "source": [
        "We'll need to convert categorical features to dummy variables using pandas! Otherwise our machine learning algorithm won't be able to directly take in those features as inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHAhz_2KRNMr"
      },
      "source": [
        "train['Embarked'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDdLNQAjRNMu"
      },
      "source": [
        "#sex = pd.get_dummies(train['Sex'],drop_first=False)\n",
        "embark = pd.get_dummies(train['Embarked'],drop_first=True)\n",
        "embark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCW-FbUCRNMy"
      },
      "source": [
        "embark.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bddAP9ZoRNM0"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "LE = LabelEncoder()\n",
        "train['Sex'] = LE.fit_transform(train['Sex'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-LgyNdjRNM3"
      },
      "source": [
        "train['Sex']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEOGLsrYRNM6"
      },
      "source": [
        "cities = [\"paris\", \"Paris\", \"tokyo\", \"amsterdam\", 'paris', 'tokyo']\n",
        "# cities_new = [city.lower() for city in cities]\n",
        "cities_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7eVdKzoRNM_"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "le.fit_transform(cities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTiTt6EnRNNB"
      },
      "source": [
        "list(le.classes_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZedzzX9lRNNF"
      },
      "source": [
        "le.transform([\"tokyo\", \"tokyo\", \"paris\", 'Paris'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLMx_O1HRNNJ"
      },
      "source": [
        "list(le.inverse_transform([2, 2, 0, 1])) #to fetch the actual labels against \n",
        "# the given numeric label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPB0ZT9HRNNO"
      },
      "source": [
        "train['Sex'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvJ7oBtYRNNS"
      },
      "source": [
        "train['Embarked'] = LE.fit_transform(train['Embarked'])\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqBOoDd-RNNZ"
      },
      "source": [
        "# Building a Logistic Regression model\n",
        "Let's start by splitting our data into a training set and test set (there is another test.csv file that you can play around with in case you want to use all this data for training)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPeGPPsXRNNZ"
      },
      "source": [
        "# Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-XVdNCSRNNa"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = train.drop('Survived', axis=1)\n",
        "Y = train['Survived']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDr9p77PRNNf"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5X3qf_jRNNk"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, \\\n",
        "                                                    test_size=0.30, \n",
        "                                                    random_state=101)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKt_Qr10RNNm"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PNm2X_qRNNp"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k92h_aBVRNNr"
      },
      "source": [
        "# Training and Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3fOPRKqRNNs"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CERnoT5IRNNv"
      },
      "source": [
        "logmodel = LogisticRegression(solver='liblinear') # , class_weight='balanced'\n",
        "logmodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxdM_0diRNNx"
      },
      "source": [
        "lm = logmodel.fit(X_train,y_train)\n",
        "lm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJR9nK-gRNN0"
      },
      "source": [
        "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
        "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
        "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
        "                   warm_start=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbiEfysuRNN3"
      },
      "source": [
        "lm.n_iter_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS8z2PXeRNN8"
      },
      "source": [
        "lm.classes_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tQVVX1hRNOB"
      },
      "source": [
        "lm.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mIgnyV2RNOF"
      },
      "source": [
        "lm.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohgKNVQ8RNOL"
      },
      "source": [
        "predictions = logmodel.predict(X_test)\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRO4IAswRNON"
      },
      "source": [
        "prob = logmodel.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPOH3qsmRNOP"
      },
      "source": [
        "prob_1 = prob[:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kPh0peDRNOT"
      },
      "source": [
        "pred = []\n",
        "for probab in prob_1:\n",
        "    if probab > 0.8:\n",
        "        pred.append(1)\n",
        "    else:\n",
        "        pred.append(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIrAumDARNOX"
      },
      "source": [
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGlrIaczRNOb"
      },
      "source": [
        "y_test[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Xb0yBrRNOe"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0OFLQtPRNOf"
      },
      "source": [
        "We can check precision,recall,f1-score using classification report!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VStI_52pRNOf"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,predictions))\n",
        "             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ACorL8dRNOh"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(y_test,predictions)\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll21_ltwRNOj"
      },
      "source": [
        "logmodel.score(X_test, y_test) #this is the accuracy score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_65ovVIRNOl"
      },
      "source": [
        "predictions[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onX8NmABRNOn"
      },
      "source": [
        "type(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhJ49FTnRNOp"
      },
      "source": [
        "df = pd.DataFrame(y_test)\n",
        "df['Predicted'] = predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT4QeBYYRNOr"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s00Y7PHDRNOt"
      },
      "source": [
        "logmodel.predict_proba(X_test[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWbAu-RxRNO1"
      },
      "source": [
        "df1 = pd.DataFrame(logmodel.coef_, columns=X.columns)\n",
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjniKChERNO3"
      },
      "source": [
        "logmodel.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzMTQWGdRNO5"
      },
      "source": [
        "Not so bad! You might want to explore other feature engineering and the other titanic_text.csv file, some suggestions for feature engineering:\n",
        "\n",
        ".Try grabbing the Title (Dr.,Mr.,Mrs,etc..) from the name as a feature\n",
        ".Maybe the Cabin letter could be a feature\n",
        ".Is there any info you can get from the ticket?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6heUobmyRNO6"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "SGD_clf = SGDClassifier()\n",
        "SGD_clf.fit(X_train, y_train)  # default loss='hinge'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpTYebLlRNO9"
      },
      "source": [
        "predictions = logmodel.predict(X_test)\n",
        "SGD_clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eecy2csSRNO_"
      },
      "source": [
        "# Load libraries\n",
        "# import pandas\n",
        "# import numpy\n",
        "# import matplotlib.pyplot as plt\n",
        "# from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.preprocessing import Imputer\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEiDl8ZGRNPE"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "# Spot Check Algorithms\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('CART', DecisionTreeClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQkVHqxkRNPG"
      },
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "results = []\n",
        "names = []\n",
        "n_splits = 5\n",
        "for name, model in models:\n",
        "    kfold = model_selection.KFold(n_splits=5, shuffle=True, \\\n",
        "                                  random_state=5)\n",
        "    cv_results = model_selection.cross_val_score(model, X_train, \\\n",
        "                                                 y_train, cv=kfold, \\\n",
        "                                                 scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %5.2f (%5.2f)\" % (name, cv_results.mean()*100, \\\n",
        "                           cv_results.std()*100)\n",
        "    print(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjuiLME1RNPK"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L7Fv_kCRNPO"
      },
      "source": [
        "results_df = pd.DataFrame(results, index=names, \\\n",
        "                          columns='CV1 CV2 CV3 CV4 CV5'.split())\n",
        "# results_df.drop(['CV Mean', 'CV Std Dev'], inplace=True, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpIcI_gxRNPR"
      },
      "source": [
        "results_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwpwSf5RRNPU"
      },
      "source": [
        "results_df['CV Mean'] = results_df.iloc[:,0:n_splits].mean(axis=1)\n",
        "results_df['CV Std Dev'] = results_df.iloc[:,0:n_splits].std(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQOgO7liRNPW"
      },
      "source": [
        "pd.set_option('precision',2)\n",
        "results_df*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1eYlnRpRNPa"
      },
      "source": [
        "results_df.sort_values(by='CV Mean', ascending=False)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyryDpqmRNPc"
      },
      "source": [
        "# InteractiveShell.ast_node_interactivity = \"last_expr\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkoZSl97RNPe"
      },
      "source": [
        "# %matplotlib inline\n",
        "# Compare Algorithms\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7Xa8joCRNPg"
      },
      "source": [
        "# Standardize the dataset\n",
        "pipelines = []\n",
        "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression())])))\n",
        "pipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()),('LDA', LinearDiscriminantAnalysis())])))\n",
        "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsClassifier())])))\n",
        "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeClassifier())])))\n",
        "pipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()),('NB', GaussianNB())])))\n",
        "pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()),('SVM', SVC())])))\n",
        "results = []\n",
        "names = []\n",
        "for name, model in pipelines:\n",
        "    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=5)\n",
        "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean()*100, cv_results.std()*100)\n",
        "#     print(msg)\n",
        "\n",
        "results_df = pd.DataFrame(results, index=names, \\\n",
        "                          columns='CV1 CV2 CV3 CV4 CV5'.split())\n",
        "results_df['CV Mean'] = results_df.iloc[:,0:n_splits].mean(axis=1)\n",
        "results_df['CV Std Dev'] = results_df.iloc[:,0:n_splits].std(axis=1)\n",
        "results_df.sort_values(by='CV Mean', ascending=False)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKdGL0bJRNPi"
      },
      "source": [
        "# Compare Algorithms\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Scaled Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWFJveJ2RNPl"
      },
      "source": [
        "# Normalize the dataset\n",
        "pipelines = []\n",
        "pipelines.append(('ScaledLR', Pipeline([('Scaler', MinMaxScaler(feature_range=(0, 1))),('LR', LogisticRegression())])))\n",
        "pipelines.append(('ScaledLDA', Pipeline([('Scaler', MinMaxScaler(feature_range=(0, 1))),('LDA', LinearDiscriminantAnalysis())])))\n",
        "pipelines.append(('ScaledKNN', Pipeline([('Scaler', MinMaxScaler(feature_range=(0, 1))),('KNN', KNearestClassifier()])))\n",
        "pipelines.append(('ScaledCART', Pipeline([('Scaler', MinMaxScaler(feature_range=(0, 1))),('CART', DecisionTreeClassifieKNeighborsClassifier())r())])))\n",
        "pipelines.append(('ScaledNB', Pipeline([('Scaler', MinMaxScaler(feature_range=(0, 1))),('NB', GaussianNB())])))\n",
        "pipelines.append(('ScaledSVM', Pipeline([('Scaler', MinMaxScaler(feature_range=(0, 1))),('SVM', SVC())])))\n",
        "results = []\n",
        "names = []\n",
        "for name, model in pipelines:\n",
        "    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=5)\n",
        "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean()*100, cv_results.std()*100)\n",
        "#     print(msg)\n",
        "\n",
        "results_df = pd.DataFrame(results, index=names, \\\n",
        "                          columns='CV1 CV2 CV3 CV4 CV5'.split())\n",
        "results_df['CV Mean'] = results_df.iloc[:,0:n_splits].mean(axis=1)\n",
        "results_df['CV Std Dev'] = results_df.iloc[:,0:n_splits].std(axis=1)\n",
        "results_df.sort_values(by='CV Mean', ascending=False)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcQUaeFpRNPm"
      },
      "source": [
        "# Compare Algorithms\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Scaled Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8IKNoJiRNPo"
      },
      "source": [
        "# ensembles\n",
        "ensembles = []\n",
        "ensembles.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()),('AB', AdaBoostClassifier())])))\n",
        "ensembles.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier())])))  \n",
        "ensembles.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier())])))\n",
        "ensembles.append(('ScaledET', Pipeline([('Scaler', StandardScaler()),('ET', ExtraTreesClassifier())])))\n",
        "results = []\n",
        "names = []\n",
        "for name, model in ensembles:\n",
        "    kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=5)\n",
        "    cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean()*100, cv_results.std()*100)\n",
        "#     print(msg)\n",
        "    \n",
        "results_df = pd.DataFrame(results, index=names, \\\n",
        "                          columns='CV1 CV2 CV3 CV4 CV5'.split())\n",
        "results_df['CV Mean'] = results_df.iloc[:,0:n_splits].mean(axis=1)\n",
        "results_df['CV Std Dev'] = results_df.iloc[:,0:n_splits].std(axis=1)\n",
        "results_df.sort_values(by='CV Mean', ascending=False)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPXEb4wdRNPs"
      },
      "source": [
        "# Compare Algorithms\n",
        "fig = plt.figure()\n",
        "fig.suptitle('Scaled Ensemble Algorithm Comparison')\n",
        "ax = fig.add_subplot(111)\n",
        "plt.boxplot(results)\n",
        "ax.set_xticklabels(names)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joGVm4ngRNPt"
      },
      "source": [
        "# Tune scaled KNN\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "rescaledX = scaler.transform(X_train)\n",
        "neighbors = [1,3,5,7,9,11,13,15,17,19,21,25,30,35,40,50]\n",
        "param_grid = dict(n_neighbors=neighbors)\n",
        "model = KNeighborsClassifier('euclidean')\n",
        "\n",
        "kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=5)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, \\\n",
        "                    scoring='accuracy', cv=kfold, )\n",
        "\n",
        "grid_result = grid.fit(rescaledX, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Y1jY2JaRNPv"
      },
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM7X7eZ_RNPx"
      },
      "source": [
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4VtMUDsRNPy"
      },
      "source": [
        "model = KNeighborsClassifier(metric='euclidean', n_neighbors=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4SA4AIRNP0"
      },
      "source": [
        "model.fit( rescaledX, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7nNXp38RNP3"
      },
      "source": [
        "c_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\n",
        "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "degree_values = [2,3,4,5]\n",
        "gamma_values =[0.1, 0.5, 1, 2]\n",
        "param_grid = dict(C=c_values, kernel=kernel_values, degree=degree_values, \\\n",
        "                 gamma = gamma_values)\n",
        "model = SVC()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90CbZScVRNP7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xBqcuTnRNP8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}